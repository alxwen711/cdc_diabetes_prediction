```{python}
import pickle
import pandas as pd
```

The data were split 70/30 into training and test sets with stratification on the target.  
Two classifiers were trained and tuned using 5-fold cross-validated grid search with **f2-score** as the scoring metric. We chose to use f2-score because it is most important to not miss true positives.

```{python}
with open("../results/models/tree_model.pickle", 'rb') as f:
        model_dt = pickle.load(f)
with open("../results/models/naive_bayes_model.pickle", 'rb') as f:
        model_nb = pickle.load(f)
result_metrics = pd.read_csv("../results/tables/model_scores.csv")

dt_max_depth = model_dt.max_depth
dt_min_samples_leaf = model_dt.min_samples_leaf
dt_f2 = result_metrics[result_metrics["Model"]=="Decision Tree"]["Test f2-score"].values[0]

nb_alpha = model_nb.named_steps["bernoullinb"].alpha
nb_f2 = result_metrics[result_metrics["Model"]=="Naive Bayes"]["Test f2-score"].values[0]
```

1. **Decision Tree** (class_weight='balanced')  
   Hyperparameters: max_depth: {6,8,10,12,14}, min_samples_leaf: {175, 200, 225, 250}  
   **Best parameters**: max_depth=`{python} dt_max_depth`, min_samples_leaf=`{python} dt_min_samples_leaf`  
   **Best CV f2-score** = `{python} dt_f2`  

2. **Bernoulli Naive Bayes** (with StandardScaler preprocessing)  
   Hyperparameters: alpha: {1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4}  
   **Best parameters**: alpha=`{python} nb_alpha`  
   **Best CV f2-score** = `{python} nb_f2`  

