```{python}
score_table = pd.read_csv("../results/tables/model_scores.csv")
```
The current performance of the Decision Tree model is likely already good enough to offer some benefit in the real world given the large number of people with undiagnosed diabetes. However, the recall score of `{python} f"{score_table.loc[0, 'Test recall']:.1%}"` could likely be improved, and the precision score of `{python} f"{score_table.loc[0, 'Test precision']:.1%}"` definitely leaves something to be desired. 

We were surprised by the high false‑positive rate in @fig-confusion-matrix but this could reflect other issues besides many “at‑risk” non‑diabetic people — for example our 0.5 threshold and optimizing F2 (which favors recall), class imbalance, poor probability calibration, model bias, or noisy/self‑reported features. To address this, we should check calibration and subgroup performance, choose an operating threshold from the precision–recall curve that reflects screening costs, and try simple fixes (other model types, a wider hyperparameter search, and targeted feature engineering). Finally, validate the model on external data and consider finding a small set of easy‑to‑obtain features for a more practical screening tool.

Some work is needed to determine this smaller number of easy to obtain features that doesn't significantly reduce model performance. Another question is whether a regression model could be made that predicts a person's risk as a percent chance of developing diabetes, though longitudinal data might be required for this type of prediction.